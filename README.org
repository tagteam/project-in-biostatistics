#+TITLE: Project in biostatistics: Targeted learning and cross-fitting
#+Author: Anders Munch & Thomas Alexander Gerds
#+LANGUAGE:  en
#+OPTIONS: H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS: TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc author:nil
#+LaTeX_CLASS: org-article
#+LaTeX_HEADER:\usepackage{authblk}
#+LaTeX_HEADER:\usepackage{natbib}
#+LaTeX_HEADER:\usepackage[table,usenames,dvipsnames]{xcolor}
#+LaTeX_HEADER:\definecolor{lightGray}{gray}{0.98}
#+LaTeX_HEADER:\definecolor{medioGray}{gray}{0.83}
#+LaTeX_HEADER:\author{Anders Munch \& Thomas Alexander Gerds}
#+LaTeX_HEADER:\affil{Department of Biostatistics, University of Copenhagen}
#+setupfile:~/emacs-genome/snps/org-templates/setup-all-purpose.org
#+superman-export-target: pdf


** Background and motivation

Let \( \mathcal{P} \) be a collection of probability measures and \( \Psi \colon
\mathcal{P} \rightarrow \R\) a pathwise differentiable parameter of interest.
For many problems, the parameter \( \Psi \) can be written as
\begin{equation*}
  \Psi(P) = P{[\phi(\blank, \nu(P))]},
\end{equation*}
where \( \nu \) is some function-valued nuisance parameter and we use the
notation \( P{[f]} = \int f \diff P \). By using tools from semiparametric
efficiency theory it is possible to choose the function $\phi$ such that valid
statistical inference for $\Psi$ can be obtained, even when $\nu$ is estimated
with data-adaptive algorithms such as machine learning
\citep{bickel1993efficient,van2011targeted,chernozhukov2018double}.

With \( \empmeas \) denoting the empirical measure of an iid.\nbsp{}sample \(
\{O_i\}_{i=1}^n \) from some \( P \in \mathcal{P}\) and $\hat{\nu}_n$ an
estimator of the nuisance parameter $\nu$, a natural estimator of $\Psi$ is
\begin{equation*}
  \hat{\Psi}_n = \empmeas{[\phi(\blank, \hat{\nu}_n)]}. 
\end{equation*}
This estimator will be consistent and asymptotically normal under a set of
general assumptions about the function \( \phi \) and the estimator
\( \hat{\nu}_n \). One assumption is that \( \hat{\nu}_n \) takes values in a
so-called \textit{Donsker class} of functions. To avoid this assumption it has
been suggested to instead employ \textit{cross-fitting}, where \( \empmeas \)
and $\hat{\nu}_n$ are constructed using separate parts of the data
\citep{chernozhukov2018double}.

Cross-fitting allows us to work under theoretically weaker assumptions at the
cost of using only a subset of the data to fit a potentially complex estimator.
Whether cross-fitting is theoretically necessary, and what the effect is in
finite sample settings is not well understood (compare, e.g.,
\cite{chen2022debiased} and \cite{zivich2021machine}).

* The project
In this project we investigate the effect of sample splitting for estimation of
the average treatment effect, which is a causal parameter of interest
\citep{hernanRobinsWhatIf}. We will examine different cross-fitted versions of
different targeted estimators, such as the targeted maximum likelihood estimator
\citep{van2006targeted,van2011targeted}, under different data-generating
mechanisms.

You will work with a random subset of the data described in
\citep{wikkelso2014prediction}. The dataset contains data from 48,272 Danish
women who gave birth twice. The main outcome is a binary variable indicating if
the woman had a postpartum haemorrhage (heavy bleeding) during the second
delivery. We use the real-world data set to guide our theoretical and numerical
studies.

In this project we touch upon elements of causal inference and targeted
learning. \cite{kennedy2016semiparametric,kennedy2022semiparametric} and
\cite{hines2022demystifying} provide good introductions to the theory of
targeted learning. The central philosophy of targeted learning is that we
clearly and explicitly define the parameter of interest that we want to
estimate. A major goal of this project is to be able to describe both your
parameter of interest and your estimator in clear and precise mathematical
terms.


