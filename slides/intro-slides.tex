% Created 2022-05-08 Sun 21:10
% Intended LaTeX compiler: pdflatex
\documentclass[smaller]{beamer}\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{array}
\usepackage[T1]{fontenc}
\usepackage{natbib}
\lstset{
keywordstyle=\color{blue},
commentstyle=\color{red},stringstyle=\color[rgb]{0,.5,0},
literate={~}{$\sim$}{1},
basicstyle=\ttfamily\small,
columns=fullflexible,
breaklines=true,
breakatwhitespace=false,
numbers=left,
numberstyle=\ttfamily\tiny\color{gray},
stepnumber=1,
numbersep=10pt,
backgroundcolor=\color{white},
tabsize=4,
keepspaces=true,
showspaces=false,
showstringspaces=false,
xleftmargin=.23in,
frame=single,
basewidth={0.5em,0.4em},
}
\usepackage{natbib, dsfont, pgfpages, tikz,amssymb, amsmath,xcolor}
\bibliographystyle{plain}
\input{./latex-settings/standard-commands.tex}
\setbeamertemplate{footline}[frame number]
\beamertemplatenavigationsymbolsempty
\usepackage{appendixnumberbeamer}
\setbeamercolor{gray}{bg=white!90!black}
\setbeamertemplate{itemize items}{$\circ$}

\renewcommand*\familydefault{\sfdefault}
\itemsep2pt
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usetheme{default}
\author{Anders Munch \& Thomas Gerds}
\date{April 25, 2022}
\title{Project in Biostatistics: Hyperparameter optimization}
\begin{document}

\maketitle
\begin{frame}[label={sec:org7c5ab8f}]{Outline of the setting}
Let $\mathcal{P}$ be a collection of probability measures over $\R^{d+2}$, so that
$O \sim P \in \mathcal{P}$, with $O = (Y, A, X)$, $Y\in \R$, $A\in \R$, and $X \in \R^d$.

\vfill

We consider estimation of a parameter $\theta \colon \mathcal{P} \rightarrow \Theta$, where $\Theta$
is either a \textbf{high-} or a \textbf{low-dimensional} space:

\vfill

\begin{enumerate}
\item Risk prediction model: For some suitable function space \(\mathcal{F}\), the parameter of interest
is \(\nu \colon \mathcal{P} \rightarrow \mathcal{F}\), \[\nu(P) = f, \text{ for } f(x, a) = \E_P[Y
   \mid X=x, A=a]. \]
\item Low-dimensional (causal) target parameter, for instance the average treatment effect: The
parameter of interest is \(\Psi \colon \mathcal{P} \rightarrow \R\),
\begin{align*}
  \Psi(P)
  & = \E_P{\left[ \E_P[Y \mid X, A=1] - \E_P[Y \mid X, A=0] \right]} \\
  & = \int {\left\{ \nu(x, 1) - \nu(x, 0) \right\}} \diff P_X(x),
    \quad \nu = \nu(P).
\end{align*}
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org6f1cc6c}]{Data}
You will work with a random subset of the data described in \cite{wikkelso2014prediction}. The
dataset contains data from 48,272 Danish women who gave birth twice. The main outcome is a binary
variable called PPH which indicates if the woman had a postpartum haemorrhage (heavy bleeding)
during the second delivery.

\vfill

\begin{block}{High- and low-dimensional inference problems}
\begin{enumerate}
\item Predicting the risk of PPH at second delivery.
\item Estimating the causal effect of a planned cesarian section on PPH.
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}[label={sec:org1887525}]{Hyperparameter and model selection/construction}
Estimation of \(\nu\) is a well-studied problem. Many (machine) learning approaches depend on one or
more hyperparameters that has to be chosen in some way. We want to examine how to do this properly,
and what the practical effects are of the different possible approaches.

\vfill

\begin{block}{Example}
One of the simplest examples of hyperparameter selection is ridge regression, which estimates \(\nu\)
with
\begin{equation*}
  \hat{\nu}_{\lambda}(x) = x^{\T} \hat{\beta}, \quad
  \hat{\beta} := \argmin_{\beta} \frac{1}{n} \sum_{i=1}^{n}L(y_i, x_i^{\T} \hat{\beta}) + \lambda
  \Vert \beta \Vert^2.
\end{equation*}

We can also consider a collection of models with different strengths to construct more robust
estimators (e.g., the Super Learner \cite{HoffmanBlog,van2011targeted}).
\end{block}
\end{frame}

\begin{frame}[label={sec:org8b899da}]{Target parameter and causal inference}
\begin{block}{Target parameter}
By explicitly defining a parameter of interest as an answer to a concrete scientific question we
ensure that we can draw relevant and meaningful conclusions from our statistical analysis.
\end{block}

\begin{block}{High-dimensional nuisance parameter}
By allowing high-dimensional nuisance parameters we avoid having to make unrealistic model
assumptions.
\end{block}

\begin{block}{Causal inference}
One example is the average treatment effect, \[\Psi(P) = \int {\left\{ \nu(x, 1) - \nu(x, 0)
\right\}} \diff P_X(x), \quad \nu=\nu(P) \in \mathcal{F}. \]

\begin{itemize}
\item How do we formalize a causal question?
\item Which assumptions are needed for the above parameter to have a causal interpretation?
\item How likely are these assumptions to hold for the data at hand?
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:orgece399c}]{Targeted inference and hyperparameter selection}
Finding a good estimator \(\hat\nu\) of \(\nu\) \alert{does \emph{not} necessarily provide a good plug-in estimator
of \(\Psi\)}! \(\rightarrow\) targeted learning and debiased ML
\citep{van2006targeted,van2011targeted,chernozhukov2018double,kennedy2022semiparametric}.

\hfill

Let \(\hat\nu_{\lambda}\) be the ridge regression estimator of \(\nu\) with penalty parameter \(\lambda\).
Let \(\hat\Psi_{\lambda}\) denote the plug-in estimator of \(\Psi\) based on \(\hat\nu_{\lambda}\), i.e.,
\small
\begin{equation*}
  \hat\Psi_{\lambda} = \frac{1}{n}\sum_{i=1}^{n}
  \left\{
    \hat{\nu}_{\lambda}(X_i, 1)- \hat{\nu}_{\lambda}(X_i, 0)
\right\}.
\end{equation*}


\begin{center}
\includegraphics[width=.9\linewidth]{./fig-nuisance-target-tuning.pdf}
\end{center}
\end{frame}

\begin{frame}[label={sec:orgc598a24}]{References}
\small\bibliography{./latex-settings/default-bib.bib}
\end{frame}
\end{document}