#+TITLE: Motivating targeted learning
#+Author: Anders Munch
#+Date: April 26, 2024

* Simulate data                                                    :noexport:

#+BEGIN_SRC R
  library(here)
  library(glmnet)
  library(data.table)
  library(ggplot2)
  library(parallel)
  setwd(here()) ## For the figures

  effect.size <- 0.2
  sim.dat <- function(n=800, p=10){
    X0 <- matrix(rnorm(n*p), nrow=n)
    A <- 1*(runif(n) < .5)
    Y <- A*effect.size + rnorm(n)
    return(data.table(Y, A, X0))
  }

  sim_ate_gform <- function(M, lambda=exp(seq(5, -10, length.out=200)), alpha=0, mc.cores = max(1,detectCores()-1), ...){
    out = do.call(rbind, mclapply(1:M, mc.cores = mc.cores, FUN = function(m){
      train = sim.dat()
      model = glmnet(train[, -1], train[,Y], alpha=alpha, lambda=lambda, ...)
      dat.copy = copy(train)
      dat.copy[, A:=0]
      fit0 = predict(model, newx=as.matrix(dat.copy[, -1]))
      dat.copy[, A:=1]
      fit1 = predict(model, newx=as.matrix(dat.copy[, -1]))
      ## Get mse for the nuisance component
      test <- sim.dat(n=10000)
      fit_test <- predict(model, newx=as.matrix(test[, -1]))
      est_out = data.table(model = "gform",
			   lambda_outcome = lambda,
			   lambda_prop = as.numeric(NA),
			   est = apply(fit1-fit0, 2, mean),
			   nuisance_mse=apply((fit_test - test[, Y])^2, 2, mean),
			   sim = m)
      return(est_out)
    }))
    return(out[])
  }
  sim_ate_cv <- function(M, alpha=0, mc.cores = max(1,detectCores()-1), ...){
    out = do.call(rbind, mclapply(1:M, mc.cores = mc.cores, FUN = function(m){
      train = sim.dat()
      model_outcome = cv.glmnet(as.matrix(train[, -1]), train[,Y], alpha=alpha,...)
      model_prop = cv.glmnet(as.matrix(train[, -(1:2)]), train[,A], alpha=alpha, family = "binomial", ...)
      dat.copy = copy(train)
      dat.copy[, A:=0]
      fit_outcome0 = predict(model_outcome, newx=as.matrix(dat.copy[, -1]), s = "lambda.min")
      dat.copy[, A:=1]
      fit_outcome1 = predict(model_outcome, newx=as.matrix(dat.copy[, -1]), s = "lambda.min")
      fit_prop = predict(model_prop, newx=as.matrix(train[, -(1:2)]), s = "lambda.min", type = "response")
      est.target = data.table(model = c("G-formula", "Debiased"),
			      lambda_outcome=model_outcome[["lambda.min"]],
			      lambda_prop=c(NA, model_prop[["lambda.min"]]),
			      est = c(mean(fit_outcome1)-mean(fit_outcome0),
				      mean(fit_outcome1)-mean(fit_outcome0) +
				      mean(train[, A]/fit_prop*(train[, Y] - fit_outcome1) -
					   (1-train[, A])/(1-fit_prop)*(train[, Y] - fit_outcome0))),
			      sim=m)
      return(est.target)
    }))
    return(out[])
  }
#+END_SRC

#+RESULTS[(2024-04-25 09:57:57) c3afb38b4057c1bd98f32969b360fd5aa4037f7a]:

#+BEGIN_SRC R
  set.seed(341)
  ate_sim_cv <- sim_ate_cv(M=1000, mc.cores=6, alpha=0)
  ate_sim_cv[,model:=factor(model,levels=c("G-formula","Debiased"),labels=c("Naive", "One-step"))]
#+END_SRC

#+RESULTS[(2024-04-25 10:43:42) a5555ca7b094b5b0e5dca40b7c669740362d0718]:
#+begin_example
         model lambda_outcome lambda_prop        est   sim
        <fctr>          <num>       <num>      <num> <int>
   1:    Naive      60.532634          NA 0.00000000     1
   2: One-step      60.532634   31.503640 0.12123246     1
   3:    Naive       1.088808          NA 0.11820633     2
   4: One-step       1.088808   31.266305 0.24991207     2
   5:    Naive       2.360686          NA 0.05711690     3
  ---                                                     
1996: One-step       2.678207   22.763616 0.22117525   998
1997:    Naive       1.398176          NA 0.09120362   999
1998: One-step       1.398176   18.953800 0.22012753   999
1999:    Naive       1.132311          NA 0.12253652  1000
2000: One-step       1.132311    4.065673 0.25509467  1000
#+end_example

* A parameter of interest
#+begin_export latex
A target parameter is a functional
\begin{equation*}
  \Psi \colon \mathcal{P} \longrightarrow \R.
\end{equation*}

\vfill

Common case that
\begin{equation*}
  \Psi(P) = P{[ \phi(\blank; \nu(P)) ]}
  = \int \phi(x; \nu(P)) P(\diff x),
\end{equation*}
where $\nu$ is a function-valued nuisance parameter such as a conditional expectation or
a conditional probability.
#+end_export


* A parameter of interest -- our setting
\small

Population of women at their second birth \citep{wikkelso2014prediction}.

\vfill

#+begin_export latex
The data consists of observations \( X = (Y, A, W) \), where \( Y \in \{0,1\}\) denotes PPH,
\( A \in \{0,1\} \) denotes planned c-section, and \( W \in \R^p \) is a vector
with information collected at the start of the second pregnancy, including
information from the first birth.

\vfill

Our parameter of interest is
\begin{align*}
  \Psi(P)
  & =
    \E_P
    {\left[
    \E_P{\left[ Y \mid A=1, W  \right]}
    - \E_P{\left[ Y \mid A=0, W  \right]}
    \right]}
  \\
  & = 
    P{[\nu(1, \blank; P) - \nu(0, \blank; P)]}    ,
\end{align*}
where
\begin{equation*}
  \nu(a, w; P) = \E_P{\left[ Y \mid A=a, W=w  \right]}.
\end{equation*}
#+end_export

\vfill

If \color{bblue}\(W\) includes all confounders of \(Y\) and \(A\)\color{black},
and there is \color{bblue}uniform positive probability of
treatment\color{black}, \Psi(P) can be interpreted as the average treatment
effect (ATE) of a planned c-section on PPH. This is known a the g-formula.

* Parametric modeling versus ML
\small
** Parametric approach
#+begin_export latex
Fit a parametric model for \( \nu \) (e.g., logistic regression). Estimate
$\Psi$ with
\begin{equation*}
  \hat{\Psi}_n^{\text{glm}} =\empmeas{[\hat{\nu}_n^{\text{glm}}(1, \blank)- \hat{\nu}_n^{\text{glm}}(0,
    \blank)]}
  = \frac{1}{n}\sum_{i=1}^{n}
  \left\{
    \hat{\nu}_n^{\text{glm}}(1, W_i) - \hat{\nu}_n^{\text{glm}}(0, W_i)
  \right\}.
\end{equation*}
#+end_export


** Nonparametric (machine learning) approach
#+begin_export latex
Fit a machine learning algorithm to learn \( \nu \) (e.g., random forest).
Estimate $\Psi$ with
\begin{equation*}
  \hat{\Psi}_n^{\text{RF}} = \empmeas{[\hat{\nu}_n^{\text{RF}}(1, \blank)- \hat{\nu}_n^{\text{RF}}(0,
    \blank)]}
  = \frac{1}{n}\sum_{i=1}^{n}
  \left\{
    \hat{\nu}_n^{\text{RF}}(1, W_i) - \hat{\nu}_n^{\text{RF}}(0, W_i)
  \right\}.
\end{equation*}
#+end_export

\vspace{.4cm}


*** gray                                        :B_beamercolorbox:
:PROPERTIES:
:BEAMER_env: beamercolorbox
:BEAMER_opt: rounded=true
:END:

\centering \normalsize What would you prefer in our setting?

* Targeted estimators

Semi-parametric efficiency theory tells us that an initial estimator can be
improved by adding an augmentation term:

#+begin_export latex
\begin{equation*}
  \hat{\Psi}_n = \hat{\Psi}_n^{\text{RF}} + \empmeas{[\dot{\psi}(\blank, \hat{P}_n)]}.
\end{equation*}
#+end_export

\vfill

The function \( \dot{\psi}(\blank, P) \) is the \color{bblue}efficient influence
function \color{black} (aka \color{bblue}canonical gradient\color{black}) of
$\Psi$.

\vfill

The term \( P{[\dot{\psi}(\blank, \hat{P}_n)]} \) can be interpreted as the
first order bias due to the estimation of \( \nu \) with \( \hat{\nu}_n^{\text{RF}}
\), and we approximate this with \( \empmeas{[\dot{\psi}(\blank, \hat{P}_n)]}
\).

* Targeted estimators -- our setting
\small

The EIF for our $\Psi$ is well-known
\citep[e.g.,][]{kennedy2016semiparametric,kennedy2022semiparametric,hines2022demystifying}
and equals
#+begin_export latex
\begin{align*}
  \dot{\psi}(X; P)
  & = \nu(W, 1; P) - \nu(W, 0;P)
    - \Psi(P)
  \\
  & \quad
    + \frac{A}{\pi(W;P)}(Y - \nu(W, 1; P))
    - \frac{1-A}{1-\pi(W;P)}(Y - \nu(W, 0;P)),
\end{align*}
where $\pi$ is the propensity score,
\begin{equation*}
  \pi(w; P) = P(A=1 \mid W=w).
\end{equation*}
#+end_export

\vfill

A targeted estimator of $\Psi$ is then
#+begin_export latex
\begin{align*}
  \hat{\Psi}_n= 
  \frac{1}{n}\sum_{i=1}^{n}
  \Big\{
  &
    \hat{\nu}_n^{\text{RF}}(1, W_i) - \hat{\nu}_n^{\text{RF}}(0, W_i)
    + \frac{A_i}{\hat{\pi}_n(W_i;P)}(Y_i - \nu(W_i, 1; P))
  \\
  & \quad
    - \frac{1-A_i}{1-\hat{\pi}_n(W_i;P)}(Y_i - \nu(W_i, 0;P))
    \Big\},
\end{align*}
where $\hat{\pi}_n$ is an estimator of $\pi$.
#+end_export



* Simulation study
\small

1. Generate a simulated data set of 800 individuals with 10 covariates.
2. Fit a ridge regression for the outcome using cross-validation to select
   to penalty parameter
3. Use the fit from step 2 to and the g-formula to estimate the ATE. We refer to
   this estimator as =naive=.
4. Fit another ridge regression for the propensity score using cross-validation
   to select to penalty parameter
5. Use the efficient influence function and the estimators from step 2 and 4 to
   target/debias the estimator calculated in step 3. We refer to this estimator
   as =one-step=.


\vfill

Repeat steps 1-5 1000 times to obtain 1000 samples of the =naive= estimator and
the =one-step= estimator.

\vfill

Examine performance of the two estimators by comparing these 1000 random samples
to the true ATE.

* Result of simulation study

#+BEGIN_SRC R :results graphics file :exports results :file naive-vs-targeted.pdf :width 6.5 :height 3.5
  ggplot(ate_sim_cv, aes(x = est)) + theme_classic() +
    geom_histogram(bins = 20, fill = "skyblue") +
    xlab("Estimate") +
    geom_vline(xintercept = effect.size, size = 1, col = "black") +
    facet_grid(~model)
#+END_SRC

#+RESULTS[(2024-04-25 13:38:34) f64b81d15aaec9feb5db5aa7b669643c5b59e898]:
[[file:naive-vs-targeted.pdf]]

* Statistical inference

To quantify uncertainty we calculate confidence intervals. This procedure relies
on an estimate of the asymptotic variance of our estimator.

\vfill


** Naive ML approach
- The use of cross-validation (and other data-adaptive algorithms) complicates
  calculation of the asymptotic variance of the =naive= estimator.

- Bootstrap is not feasible in practice (and works poorly with cross-validation)

** One step estimator
- Closed form expression for the asymptotic variance which can be estimated
  based on the models we have already fitted.


* References
\footnotesize \bibliography{bib.bib}

* Setup                                                            :noexport:

#+LANGUAGE:  en
#+OPTIONS:   H:1 num:t toc:nil ':t ^:t
#+startup: beamer
#+LaTeX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [smaller]
#+LaTeX_HEADER: \usepackage{natbib, dsfont, pgfpages, tikz,amssymb, amsmath,xcolor}
#+LaTeX_HEADER: \bibliographystyle{abbrvnat}
#+BIBLIOGRAPHY: bib plain

# Beamer settins:
#+LaTeX_HEADER: \setbeamertemplate{footline}[frame number]
#+LaTeX_HEADER: \beamertemplatenavigationsymbolsempty
#+LaTeX_HEADER: \usepackage{appendixnumberbeamer}
#+LaTeX_HEADER: \setbeamercolor{gray}{bg=white!90!black}
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+LATEX_HEADER: \setbeamertemplate{itemize items}{$\circ$}

# Setting size of code block
#+LaTeX_HEADER: \lstset{basicstyle=\ttfamily\footnotesize}
# Using when output of code is verbatim
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\footnotesize}

# Matching beamer blue color
#+LaTeX_HEADER: \definecolor{bblue}{rgb}{0.2,0.2,0.7}

# Common command
#+LaTeX_HEADER: \newcommand{\E}{{\ensuremath{\mathop{{\mathbb{E}}}}}} 
#+LaTeX_HEADER: \newcommand{\R}{\mathbb{R}}
#+LaTeX_HEADER: \newcommand{\N}{\mathbb{N}}
#+LaTeX_HEADER: \newcommand{\blank}{\makebox[1ex]{\textbf{$\cdot$}}}
#+LaTeX_HEADER: \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
#+LaTeX_HEADER: \def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
#+LaTeX_HEADER: \renewcommand{\phi}{\varphi}
#+LaTeX_HEADER: \renewcommand{\epsilon}{\varepsilon}
#+LaTeX_HEADER: \newcommand*\diff{\mathop{}\!\mathrm{d}}
#+LaTeX_HEADER: \newcommand{\weakly}{\rightsquigarrow}
#+LaTeX_HEADER: \newcommand\smallO{\textit{o}}
#+LaTeX_HEADER: \newcommand\bigO{\textit{O}}
#+LaTeX_HEADER: \newcommand{\midd}{\; \middle|\;}
#+LaTeX_HEADER: \newcommand{\1}{\mathds{1}}
#+LaTeX_HEADER: \usepackage{ifthen} %% Empirical process with default argument
#+LaTeX_HEADER: \newcommand{\G}[2][n]{{\ensuremath{\mathbb{G}_{#1}}{\left[#2\right]}}}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{\arg\!\min}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{\arg\!\max}
#+LaTeX_HEADER: \newcommand{\V}{\mathrm{Var}} % variance
#+LaTeX_HEADER: \newcommand{\eqd}{\stackrel{d}{=}} % equality in distribution
#+LaTeX_HEADER: \newcommand{\arrow}[1]{\xrightarrow{\; {#1} \;}}
#+LaTeX_HEADER: \newcommand{\arrowP}{\xrightarrow{\; P \;}} % convergence in probability
#+LaTeX_HEADER: \newcommand{\KL}{\ensuremath{D_{\mathrm{KL}}}}
#+LaTeX_HEADER: \newcommand{\leb}{\lambda} % the Lebesgue measure
#+LaTeX_HEADER: \DeclareMathOperator{\TT}{\Psi} % target parameter
#+LaTeX_HEADER: \newcommand{\empmeas}{\ensuremath{\mathbb{P}_n}} % empirical measure
